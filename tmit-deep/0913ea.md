#Backprop

* a feature-öket mindig normalizálni kell (ökölszabályként, várható érték,
  szórás)

##elemi neuron

* jelölés: 
  * `s = \sum w_i x_i`
  * aktiváció: a(s)
    * a = sgn, tanh, relu, PReLU, leaky ReLU
    * sigmoid = 1 / (1 + exp(-s)
    * tanh = (exp(s) - exp(-s)) / (exp(s) + exp(-s)) 
* `w_{ij}^(k)`: a k-adik rétegben az i-edik neuronból a j-edikbe menő súly
* szintetikus gradiens by deep minds
  * hogy külön lehessen osztani rétegeket
  * (elválasztó) rétegenként egy külön előrecsatolt neuronháló, ami az gradienst közelíti
    
