                                            _Az élet úgy hozza, vagy bármi más_

#Napirend előtt

##La révolution de l'intelligence artificielle

Montreáli Egyetem, 
Bengio, 
LeCun, 
  * conv netek atyja
Pineau 

##2. kisházi

* lehetőleg együtt is működjenek az algók

#Text-to-speech (TTS)

* példa arra, hogy milyen problémák vannak ffwd hálókkal
* áttekintés
  * szöveg előfeldolgozása
    * tokenizálás (mondat, szó)
    * normalizálás (számjegyeket kiírjuk, rövidítést feloldjuk, különleges
      írásjelet kiszedjük)
    * cimkézés (POS, hangsúly)
    * idegen szavak átírása
  * beszéd-szintézis
    * prozódia-predikció
      * f0 alapfreq
      * hagnsúly, hangok hossza, magassága, energia
    * hullámforma-generálás
* felolvasók típusai
  1. artikulációs
  2. formánsszintézis
  3. elemkiválasztásos 
    * útkeresés gráfban, 
    * ipari rszekben ma is
    * pl. Keleti pu: domainspec
  4. stat param (1995+, 2005+)
    * hmm
    * dnn
      * ffwd
      * lstm
      * WaveNet -- két hete 
* stat param
  * hangszín lemodellezhető 10 perc beszéd alapján
* hmm-tts
  * beszédkorpusz -- nem trivi beszerezni
  * gerjesztési param: f0 
  * TBP szerint a hmm-mel az is baj, hogy nehéz megérteni
  * beszédhang-5-gram (kvinfón)
* bemenet: kontextusfüggő címkék (full context labels)
  * beszédhang
  * szótag, hangsúlyos-e, trigram, beszédhangok száma, stb.
  * szó
  * mondatrész
  * mondat
* WaveNet
  * Deep Mind, beszédtechnológus csak egy volt a csapatban
  * PixelRNN, PixelCNN, képgenerálás: pixel a környezete alapján
  * conv net, gyorsabban tanul, mint egy rnn
  * nem MSE (regresszió), hanem cross entropy, kvantálás ==> osztályozás
  * gated activation func hangra jobb, mint a ReLU

##pataméterek

* f0 kinyerésére program pl. swipe
* mfcc helyett Mel Generalized Cepstrum
* nehézség: az f0 nélküli részeken az interpol fölösleges adatot hoz be
* zöngés-zöngétlen hiba: ,,légies"

##dnn-nel

* bement: a hmm-nek előkészített címkék
* kimenetet 0-1 közé normalizáljuk
* lehet cnn, rnn, fcnn is
* súly init: Xavier Gorot and Y Bengio 2010
  * emiatt lehetett 2005-ben (RBM) elhagyni az előtanulást
  * amire a vanishing gradient (VG) miatt volt szükség (sigmoid, tanh)
  * VG nem csak RNN-nél van?
  * ReLU, LReLU, PReLU
    * gyors, sparse activ, 
    * parametric ReLU: a negatív rész is növekvő
    * softplus ln(1+exp x)

##input és output

* beszédhang-5-gramok one-hot kódolása
  * one-hot encoding sklearnben...
* csomó jegy 
  * miért kell ilyen bonyolultan?
* időzítés

#torch-ban

* hdf5 pickle helyett
* sptk, pysptk
* standardizálás, normalizálás csak a tesztadaton
* `collectgarbage() # hogy ne teljen meg hamar a memória`
* optimMethod
  * lehet megadni adagradot sgd-t is

* eredmények rögzítése: 
  * naplózzuk, hogy milyen hiperparaméternél mi volt az eredmény




