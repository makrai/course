#3 Regularizáció

* Dropoutot megérteni!
* a learnin rate
  * -nek van a legnagyobb hatása
  * van egy két nagyságrendnyi tartomány, ahol jó
  * decay
    * lineáris
    * AdaGrad
      `eta = 1 / \sqrt \sum_{i=1}^t (\grad_x f)_{i}^2 + eta_0`
      az eddigi update-ek összegével kell osztani (komponensenként)
      \grad ... _i: az i-edik lépésben mennyi volt a grad
    * momentum, adam
* gradient clippinget is szoktak nem rekurrensnél is

* idegháló szemléletes debuggolása:
  ideghálóból döntési fa (,,szinaptikus háló")
